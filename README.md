# ğŸ›  <u>Machine learning</u>

Machine Learning is a branch of Artificial Intelligence (AI) that enables computers to learn patterns and make decisions or predictions from data without being explicitly programmed.

### ğŸ§· <u>*TYPES OF MACHINE LEARNING*</u>
1. Supervised Learning
2. Unsupervised Learning
3. Semi-Supervised Learning
4. Reinforcement Learinig

---

## ğŸ…° <u>*SUPERVISED LEARNING*</u>

> Model learns from labeled data (data with input-output pairs).

### ğŸ¯ Goal:
To predict outcomes for new, unseen data based on past labeled examples.

### ğŸ“˜ Definition:
In supervised learning, the algorithm is trained using a dataset that contains input features (X) and corresponding output labels (Y).
It learns the mapping function f: X â†’ Y.

### ğŸ§© Examples:
1. Predicting house prices ğŸ 
2. Email spam detection ğŸ“§
3. Disease diagnosis from medical images ğŸ©º
4. Stock price prediction ğŸ“Š
5. Weather forecasting ğŸŒ¦
6. Credit card fraud detection ğŸ’³
7. Sentiment analysis on reviews ğŸ’¬
8. Handwritten digit recognition âœ
9. Predicting student grades ğŸ“

### âš™ Techniques / Algorithms:

    Supervised Learning
    â”‚
    â”œâ”€â”€ Regression (Predicting Continuous Output)
    â”‚   â”œâ”€â”€ Linear Regression
    â”‚   â”œâ”€â”€ Polynomial Regression
    â”‚   â”œâ”€â”€ Ridge Regression
    â”‚   â”œâ”€â”€ Lasso Regression
    â”‚   â””â”€â”€ Support Vector Regression (SVR)
    â”‚
    â””â”€â”€ Classification (Predicting Categorical Output)
        â”œâ”€â”€ Logistic Regression
        â”œâ”€â”€ Decision Tree
        â”œâ”€â”€ Random Forest
        â”œâ”€â”€ Support Vector Machine (SVM)
        â”œâ”€â”€ Naive Bayes
        â”œâ”€â”€ K-Nearest Neighbors (KNN)
        â””â”€â”€ Neural Networks


---
## ğŸ…± <u>*UNSUPERVISED LEARNING*</u>

> Model learns from unlabeled data (only inputs, no outputs).

### ğŸ¯ Goal:
To find hidden patterns, structures, or relationships in data.

### ğŸ“˜ Definition:
In unsupervised learning, the model explores the dataâ€™s structure without explicit labels, discovering patterns or grouping similar data points.

### ğŸ§© Examples:

1. Customer segmentation in marketing ğŸ‘¥
2. Grouping news articles by topic ğŸ—
3. Market basket analysis (product association) ğŸ›’
4. Anomaly detection (fraud, defects)
5. Image compression or clustering ğŸ–¼
6. Topic modeling in text data ğŸ§¾
7. Social network analysis ğŸŒ
8. Gene sequence clustering ğŸ§¬
9. Recommender system (unsupervised embeddings) ğŸ§

### âš™ Techniques / Algorithms:

    Unsupervised Learning
    â”‚
    â”œâ”€â”€ Clustering (Grouping similar data)
    â”‚   â”œâ”€â”€ K-Means Clustering
    â”‚   â”œâ”€â”€ Hierarchical Clustering
    â”‚   â”œâ”€â”€ DBSCAN
    â”‚   â””â”€â”€ Mean Shift
    â”‚
    â””â”€â”€ Dimensionality Reduction (Simplifying data)
        â”œâ”€â”€ Principal Component Analysis (PCA)
        â”œâ”€â”€ t-SNE
        â”œâ”€â”€ Autoencoders
        â””â”€â”€ Singular Value Decomposition (SVD)

---
## Â© <u>*SEMI-SUPERVISED LEARNING*</u>

> Model uses a mix of labeled and unlabeled data.

### ğŸ¯ Goal:
To improve learning accuracy when obtaining labeled data is expensive or limited.

### ğŸ“˜ Definition:
Combines the strengths of supervised and unsupervised learning â€” the model first learns from the few labeled samples, then extracts patterns from the unlabeled ones.

### ğŸ§© Examples:

1. Web page classification ğŸŒ
2. Medical image labeling with few labeled scans ğŸ©»
3. Speech recognition with limited transcribed audio ğŸ™
4. Email categorization ğŸ“¬
5. Fraud detection ğŸ’¸
6. Protein structure prediction ğŸ§«
7. Sentiment analysis with small labeled datasets ğŸ’­
8. Object recognition in photos ğŸ“·
9. Customer behavior prediction ğŸ‘¤

### âš™ Techniques / Algorithms:

    Semi-Supervised Learning
    â”‚
    â”œâ”€â”€ Self-Training
    â”œâ”€â”€ Co-Training
    â”œâ”€â”€ Graph-Based Methods
    â”œâ”€â”€ GenerativeÂ ModelsÂ (e.g., Variational Autoencoders)
    â””â”€â”€ Semi-Supervised Support Vector Machines (S3VM)

---
## â–¶ <u>*REINFORCEMENT LEARNING (RL)*</u>

> Model learns by interacting with an environment and receiving rewards or penalties.

### ğŸ¯ Goal:
To learn a sequence of actions that maximize cumulative reward over time.


### ğŸ“˜ Definition:
An agent takes actions in an environment to achieve a goal, receiving feedback (reward or punishment) that helps it learn optimal strategies.

### ğŸ§© Examples:

1. Game-playing AI (Chess, Go, Atari) ğŸ®
2. Robotics and autonomous navigation ğŸ¤–
3. Self-driving cars ğŸš—
4. Dynamic pricing systems ğŸ’°
5. Traffic signal control ğŸš¦
6. Recommendation systems ğŸ¬
7. Resource allocation in networks ğŸŒ
8. Stock trading bots ğŸ“ˆ
9. Industrial process automation âš™



### âš™ Techniques / Algorithms:

    Reinforcement Learning
    â”‚
    â”œâ”€â”€ Model-Free Methods
    â”‚   â”œâ”€â”€ Q-Learning
    â”‚   â”œâ”€â”€ Deep Q-Networks (DQN)
    â”‚   â”œâ”€â”€ SARSA
    â”‚   â””â”€â”€ Policy Gradient Methods
    â”‚
    â””â”€â”€ Model-Based Methods
        â”œâ”€â”€ Monte Carlo Tree Search (MCTS)
        â”œâ”€â”€ Markov Decision Processes (MDP)
        â””â”€â”€ Actor-Critic Methods (A2C, DDPG)


---

## ğŸ“§ <u>*SELF-SUPERVISED LEARNING (Emerging Type)*</u>

> Model generates its own labels from data itself.

### ğŸ¯ Goal:
To enable learning from large amounts of unlabeled data efficiently.

### ğŸ“˜ Definition:
Self-supervised learning uses parts of the input as supervision for other parts (e.g., predicting missing words or image patches).

### ğŸ§© Examples:

1. Predicting missing words in a sentence (e.g., BERT) ğŸ§ 
2. Predicting next frame in a video ğŸ¥
3. Colorizing black-and-white images ğŸ–¤â¡ğŸ¨
4. Predicting rotation of images ğŸŒ€
5. Sentence embedding generation ğŸ—£
6. Masked autoencoding ğŸ§©
7. Contrastive learning (SimCLR, MoCo) âš¡
8. Speech representation learning ğŸ§
9. Code completion in IDEs ğŸ’»

### âš™ Techniques / Algorithms:

    Self-Supervised Learning
    â”‚
    â”œâ”€â”€ Contrastive Learning (SimCLR, MoCo)
    â”œâ”€â”€ Masked Language Modeling (BERT)
    â”œâ”€â”€ Predictive Coding
    â”œâ”€â”€ Autoencoders
    â””â”€â”€ Generative Models (GPT, Vision Transformers)


---

## ğŸŒ² <u>Summary Tree (Condensed Overview)</u>

       MACHINE LEARNING
       â”‚
       â”œâ”€â”€ Supervised Learning
       â”‚   â”œâ”€â”€ Regression â†’ (Linear, Lasso, Ridge, SVR)
       â”‚   â””â”€â”€ Classification â†’ (Decision Tree, SVM, KNN, RF)
       â”‚
       â”œâ”€â”€ Unsupervised Learning
       â”‚   â”œâ”€â”€ Clustering â†’ (K-Means, DBSCAN, Hierarchical)
       â”‚   â””â”€â”€ Dimensionality Reduction â†’ (PCA, t-SNE, Autoencoders)
       â”‚
       â”œâ”€â”€ Semi-Supervised Learning
       â”‚   â””â”€â”€ (Self-Training, Co-Training, S3VM, Graph Models)
       â”‚
       â”œâ”€â”€ Reinforcement Learning
       â”‚   â”œâ”€â”€ Model-Free â†’ (Q-Learning, DQN, SARSA)
       â”‚   â””â”€â”€ Model-Based â†’ (MDP, Actor-Critic, MCTS)
       â”‚
       â””â”€â”€ Self-Supervised Learning
       â””â”€â”€ (BERT, GPT, SimCLR, Autoencoders)

       
